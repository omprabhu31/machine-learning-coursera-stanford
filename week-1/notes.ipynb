{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Week 1 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E\" - Tom Mitchell\n",
    "\n",
    "For example, consider a weather prediction algorithm which takes from historical weather data to predict future weather:\n",
    "\n",
    "* E = The process of the algorithm examining a large amount of historical weather data\n",
    "* T = The weather prediction task\n",
    "* P = The probability of it correctly predicting a future date's weather\n",
    "\n",
    "#### Supervised Learning\n",
    "\n",
    "*Supervised learning* is the task of finding out a potential relationship between input and output variables given a labelled dataset to learn from (i.e. make an \"informed\" prediction). SL problems can be broadly categorized into:\n",
    "\n",
    "* regression - predict results within a continuous output (predicting tomorrow's average temperature in degrees Celsius; predicting the price of houses given a record of previous house prices vs house area)\n",
    "* classification - predict results within a discrete output (predicting tomorrow's weather as being either Stormy, Cloudy or Rainy; predicting whether or not a house will sell for a given price and area based on a labelled training set)\n",
    "\n",
    "#### Unsupervised Learning\n",
    "\n",
    "In *unsupervised learning*, datasets do not have labelled outputs and predictions are often made by clustering data based on relationships among variables.\n",
    "\n",
    "* clustering - given a large set of spam mails, figure out if there are different types of spam\n",
    "* non-clustering - [the cocktail party problem](https://en.wikipedia.org/wiki/Cocktail_party_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation, Cost Function and Gradient Descent\n",
    "\n",
    "| Notation | Meaning |\n",
    "| --- | --- |\n",
    "| $m$ | number of training examples |\n",
    "| $X, Y$ | space of input and output values |\n",
    "| $x^{(i)}$ | the $i$-th input variable |\n",
    "| $(x^{(i)}, y^{(i)})$ | the $i$-th input-output pair ($i = 1, 2, \\dots, m$) |\n",
    "\n",
    "A supervised learning problem can be better posed as - \"given a training set, learn a function $h: X \\to Y$\". We start small, with approximating $h$ as a linear function (linear regression), then move on to more complicated functions\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "For $h_{\\theta}(x) = \\theta_0 + \\theta_1x$, we want to choose parameters $\\theta_0, \\theta_1$ such that $h_{\\theta}(x)$ is close to $y$ for $(x,y)$. The mean squared error is a common choice for the *cost function*, though there can be other contenders as well.\n",
    "\n",
    "| Terminology | Expression |\n",
    "| --- | --- |\n",
    "| hypothesis | $h_{\\theta}(x) = \\theta_0 + \\theta_1x$ |\n",
    "| parameters | $\\theta_0, \\theta_1$ |\n",
    "| cost function | $J(\\theta_0, \\theta_1) = \\displaystyle\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$ |\n",
    "| goal | optimum $\\theta_0, \\theta_1$ to minimize $J(\\theta_0, \\theta_1)$ |\n",
    "\n",
    "For a better visualization of the cost function, refer to [this image](https://github.com/omprabhu31/machine-learning-coursera-stanford/blob/main/week-1/images/cost_function_visualization.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Imagine standing on a hilly terrain at a random point. You want to descend down the hill as fast as possible - what do you do? You look around for the direction where the downward slope is maximum and take a teensy little step. Note that the direction of maximum descent may now have changed, so you turn appropriately and continue descending using the same technique. \n",
    "\n",
    "That is essentially what *gradient descent* is - of course, you could end up at completely different points depending on where you started (i.e. gradient descent might not always result in a global optimum).\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1)\\to\\left\\lbrace\n",
    "\\begin{array}{l}\n",
    "    \\texttt{temp0} := \\theta_0 - \\alpha\\dfrac{\\partial}{\\partial\\theta_0}J(\\theta_0, \\theta_1)\\\\\n",
    "    \\texttt{temp1} := \\theta_1 - \\alpha\\dfrac{\\partial}{\\partial\\theta_1}J(\\theta_0, \\theta_1)\\\\\n",
    "    \\theta_0 := \\texttt{temp0}\\\\\n",
    "    \\theta_1 := \\texttt{temp1}\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "The size of your teensy little steps might differ based on the size of your leg - this is analogous to what we call the *learning rate* (denoted by $\\alpha$) in gradient descent. If it is too small, gradient descent will be very slow, while a large $\\alpha$ might overshoot (and possible even diverge after) the minimum.\n",
    "\n",
    "#### Gradient Descent for Linear Regression\n",
    "\n",
    "$$\n",
    "\\left.\n",
    "\\begin{array}{r}\n",
    "    \\displaystyle h_{\\theta}(x) = \\theta_0 + \\theta_1x\\\\\n",
    "    \\displaystyle J(\\theta_0, \\theta_1) = \\displaystyle\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2\\\\\n",
    "    \\displaystyle \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1)\n",
    "\\end{array}\n",
    "\\right\\rbrace\\to\\left.\n",
    "\\begin{array}{l}\n",
    "    \\displaystyle \\dfrac{\\partial}{\\partial\\theta_0}J(\\theta_0, \\theta_1) = \\displaystyle\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})\\\\\n",
    "    \\displaystyle \\dfrac{\\partial}{\\partial\\theta_0}J(\\theta_0, \\theta_1) = \\displaystyle\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}\n",
    "\\end{array}\n",
    "\\right\\rbrace\\to\n",
    "\\begin{array}{l}\n",
    "    \\displaystyle \\theta_0 := \\theta_0 - \\displaystyle\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})\\\\\n",
    "    \\displaystyle \\theta_1 := \\theta_1 - \\displaystyle\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "While we would normally have to worry about gradient descent possibly resulting in a local optimum, it turns out that the cost function for linear regression is always convex (or bowl-shaped) - i.e. gradient descent for linear regression always results in a global optimum.\n",
    "\n",
    "Additionally, this is also an example of \"batch\" gradient descent, since every step of gradient descent uses all the training examples. For a visual representation of gradient descent for linear regression, refer to [this image](https://github.com/omprabhu31/machine-learning-coursera-stanford/blob/main/week-1/images/cost_function_visualization.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "A *matrix* is a rectangular array of numbers, whose dimension/order is expressed as $n_{\\text{rows}}\\times n_{\\text{columns}}$. A real matrix of order $m\\times n$ is often represented as $\\mathbb{R}^{m\\times n}$. For a matrix $A$, $A_{ij}$ (sometimes expressed as $a_{ij}$) refers to the term present in the $i$-th row and $j$-th column.\n",
    "\n",
    "A *vector* is a special kind of matrix with only one column (or one row, called a row vector). A vector of dimension $n$ is essentially a $n\\times 1$ matrix, and is represented at $\\mathbb{R}^n$. For a vector $y$, $y_i$ refers to the $i$-th element of the vector. A 1-indexed vector begins with the element $y_1$, while a 0-indexed vector begins with the element $y_0$.\n",
    "\n",
    "#### Elementary Matrix Operations\n",
    "\n",
    "Matrix addition and subtraction is carried out by individually adding/subtracting elements at the same index in each of the matrices. Consequently, matrices of different dimensions cannot be added or subtracted.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    a_1 & b_1\\\\\n",
    "    c_1 & d_1\\\\\n",
    "\\end{bmatrix} \\pm \\begin{bmatrix}\n",
    "    a_2 & b_2\\\\\n",
    "    c_2 & d_2\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    a_1 \\pm a_2 & b_1 \\pm b_2\\\\\n",
    "    c_1 \\pm c_2 & d_1 \\pm d_2\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Scalar multiplication and division of a matrix is carried out by multiplying/dividing each of the elements of the matrix by a scalar. In case of scalar addition or subraction, we convert the scalar into a matrix with each element equal to the scalar before carrying out the operation.\n",
    "\n",
    "$$k\\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\\\\\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "    ka & kb\\\\\n",
    "    kc & kd\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\\\\\n",
    "\\end{bmatrix} \\pm k=\\begin{bmatrix}\n",
    "    a \\pm k & b \\pm k\\\\\n",
    "    c \\pm k & d \\pm k\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "#### Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is slightly more involved. For two matrices to be multiplicable, the number of columns in the first matrix must be equal to the number of rows in the second matrix. For two matrices $A$ and $B$ of order $m\\times n$ and $n\\times p$ respectively,\n",
    "\n",
    "$$A\\times B = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn} \n",
    "\\end{bmatrix} \\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "b_{n1} & b_{n2} & \\cdots & b_{np} \n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^n(a_{1i}b_{i1}) & \\sum_{i=1}^n(a_{1i}b_{i2}) & \\cdots & \\sum_{i=1}^n(a_{1i}b_{ip})\\\\\n",
    "\\sum_{i=1}^n(a_{2i}b_{i1}) & \\sum_{i=1}^n(a_{2i}b_{i2}) & \\cdots & \\sum_{i=1}^n(a_{2i}b_{ip})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_{i=1}^n(a_{mi}b_{i1}) & \\sum_{i=1}^n(a_{mi}b_{i2}) & \\cdots & \\sum_{i=1}^n(a_{mi}b_{ip})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The problem of finding the values of the hypotheses of a set of input variables (or even comparing several competing hypotheses) can be reduced to a matrix-vector multiplication problem as follows (this will make our further code implementations shorter to write as well as computartionally faster):\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    1 & x^{(1)}\\\\\n",
    "    1 & x^{(1)}\\\\\n",
    "    \\vdots & \\vdots\\\\\n",
    "    1 & x^{(m)}\n",
    "\\end{bmatrix}\\times \n",
    "\\begin{bmatrix}\n",
    "    \\theta_0\\\\\n",
    "    \\theta_1\n",
    "\\end{bmatrix}\\longrightarrow \n",
    "\\begin{bmatrix}\n",
    "    1 & x^{(1)}\\\\\n",
    "    1 & x^{(1)}\\\\\n",
    "    \\vdots & \\vdots\\\\\n",
    "    1 & x^{(m)}\n",
    "\\end{bmatrix}\\times \n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0a} & \\theta_{0b} & \\cdots & \\theta_{0n}\\\\\n",
    "    \\theta_{1a} & \\theta_{1b} & \\cdots & \\theta_{1n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Matrix multiplication has several properties which are lsiten below:\n",
    "* not commutative - in general, $A\\times B \\neq B\\times A$\n",
    "* associative - $A\\times B \\times C = A\\times (B\\times C) = (A\\times B)\\times C$\n",
    "* identity matrix - $A\\times I = I\\times A = A$\n",
    "\n",
    "#### Inverse and Transpose of a Matrix\n",
    "\n",
    "For a $n$-th order square matrix $A$ with [determinant](https://en.wikipedia.org/wiki/Determinant) not equal to zero (i.e. non-singular matrix), the inverse $A^{-1}$ exists such that $A\\times A^{-1} = I_{n\\times n}$.\n",
    "\n",
    "The transpose of a matrix $A$ (denoted by $A^T$) is expressed by inverting the row and column terms, i.e. $A_{ij}^T = A_{ji}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
