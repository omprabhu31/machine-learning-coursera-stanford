{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Week 2 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "More often than not, we will deal with problems involving a lot more than just one input feature. The updated notation is as follows ($m$, $y$ remain the same):\n",
    "\n",
    "| Notation | Meaning |\n",
    "| -------- | ------- |\n",
    "| $n$ | number of features |\n",
    "| $x^{(i)}$ | input features of the $i$-th training example |\n",
    "| $x_j$ | $j$-th input feature |\n",
    "| $x_j^{(i)}$ | value of feature $j$ in $i$-th training example |\n",
    "\n",
    "The modified hypothesis is expressed as $h_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_nx_n$. For $x_0$ such that $x_0^{(i)} = 1$\n",
    "\n",
    "$$x = \\begin{bmatrix}\n",
    "    x_0 & x_1 & \\cdots & x_n\n",
    "\\end{bmatrix}^T\\hspace{2mm};\\hspace{2mm}\n",
    "\\theta = \\begin{bmatrix}\n",
    "    \\theta_0 & \\theta_1 & \\cdots & \\theta_n\n",
    "\\end{bmatrix}^T\\implies h_{\\theta}(x) = \\theta^Tx = x^T\\theta$$\n",
    "\n",
    "#### Gradient Descent for Multiple Variables\n",
    "\n",
    "| Terminology | Expression |\n",
    "| --- | --- |\n",
    "| hypothesis | $h_{\\theta}(x) = \\theta^Tx = \\theta_0 + \\theta_1x + \\dots + \\theta_nx_n$ |\n",
    "| parameters | $\\theta_0, \\theta_1, \\dots, \\theta_n$ |\n",
    "| cost function | $J(\\theta_0, \\theta_1, \\dots, \\theta_n) = J(\\theta) = \\displaystyle\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$ |\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta)\\to\\left\\lbrace\n",
    "\\begin{array}{l}\n",
    "    \\displaystyle \\theta_0 := \\theta_0 - \\displaystyle\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})\\\\\n",
    "    \\displaystyle \\theta_j := \\theta_j - \\displaystyle\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\\hspace{10mm}(j = 0, 1, \\dots, n)\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "#### Feature Scaling\n",
    "\n",
    "Let's say, for example, we are dealing with a price analysis system for housing with size ($x_1$: between 0-2000 sqft) and number of bedrooms ($x_2$: between 1-5) as the input features. The contour plot of $\\theta_1$ vs $\\theta_2$ would be highly skewed as even a slight change in $\\theta_1$ could have a very high influence on the hypothesis, but not so much for $\\theta_2$. This would lead to gradient descent taking a very large time to converge.\n",
    "\n",
    "To avoid this, we use *feature scaling* by re-defining the features as, say, $x_1 = \\frac{\\text{size}}{2000},$ $x_2 = \\frac{\\text{number of bedrooms}}{5}$. More commonly, we try to get every feature into approximately a $-1\\leq x_i\\leq 1$ range.\n",
    "\n",
    "Sometimes we may also want to carry out *mean normalization*, i.e. replace $x_i$ with $x_i-\\mu_i$ (except $x_0$) to make features have approximately have zero mean. For our example, we may define $x_1 = \\frac{\\text{size}-1000}{2000}$, $x_2 = \\frac{\\text{number of bedrooms}-2}{5}$.\n",
    "\n",
    "#### Convergence Tests\n",
    "\n",
    "In general, it is always a good idea to plot $J(\\theta)$ vs number of iterations to check whether gradient descent is working where the curve flattens out. One may also employ automatic convergence tests (i.e. stop gradient descent if $J(\\theta)$ decreases by less than the specified value in one iteration).\n",
    "\n",
    "In some cases, gradient descent may fail to converge or may even diverge - a very high learning rate $\\alpha$ is a most likely the reason for this. A common practice is to try out various powers of 10 as values for $\\alpha$ to come upon a reasonably quick as well as accurate implementation of gradient descent.\n",
    "\n",
    "#### Polynomial Regression\n",
    "\n",
    "A typical dataset that could be better tackled using polynomial regression may look like [this](https://github.com/omprabhu31/machine-learning-coursera-stanford/blob/main/week-2/images/polynomial_regression.png). Depending on the dataset, you may want to fit the data to a quadratic, cubic or maybe even an exponential hypothesis function. \n",
    "\n",
    "For example, a housing price prediction algorithm based on house size may have the hypothesis $h_{\\theta}(x) = \\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3$. The mechanism of a linear regression model allows us to define features $x_1 = (size)$, $x_2=(size)^2$ and $x_3=(size)^3$ and then carry out linear regression as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "\n",
    "The *normal equation* presents an analytical method to solve for the parameters $\\theta$, which may be easier than gradient descent in some cases. Imagine a 1-dimensional problem, where $\\theta$ is not even a vector, where $J(\\theta) = a\\theta^2 + b\\theta +c$. We could just equate the first derivative of $J$ to zero and find out the value of $\\theta$. However, this becomes cumbersome as the dimension of $\\theta$ increases.\n",
    "\n",
    "| Gradient Descent | Normal Equation |\n",
    "| ---------------- | --------------- |\n",
    "| need to choose $\\alpha$ | no need to choose $\\alpha$ |\n",
    "| needs many iterations | no need to iterate |\n",
    "| order $\\mathcal{O}(kn^2)$ | $\\mathcal{O}(n^3)$, need to compute $(X^TX)^{-1}$ |\n",
    "| works well even when $n$ is large | slow if $n$ is very large |\n",
    "\n",
    "For a general problem with $n$ features and $m$ training examples, the optimum value of $\\theta$ is given by $\\theta = (X^TX)^{-1}X^Ty$. Further, it also turns out that feature scaling is not necessary for the normal equation method.\n",
    "\n",
    "$$\\left.\\begin{array}{rl}\n",
    "    n\\text{ input features} \\hspace{3mm} & \\hspace{3mm}\\text{feature vectors } x^{(i)} = \\begin{bmatrix} x_0^{(i)}\\\\ x_1^{(i)}\\\\ \\vdots \\\\ x_n^{(i)}\\end{bmatrix}\\\\\n",
    "    m\\text{ training examples}\\hspace{3mm} & \\hspace{3mm}\n",
    "        \\text{design matrix }X=\\begin{bmatrix}\n",
    "            \\cdots & (x^{(1)})^T & \\cdots\\\\\n",
    "            \\cdots & (x^{(2)})^T & \\cdots\\\\\n",
    "             & \\vdots & \\\\\n",
    "            \\cdots & (x^{(m)})^T & \\cdots\\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{array}\\right\\rbrace\\to\\theta = (X^TX)^{-1}X^Ty\\to\\left\\lbrace\\begin{array}{l}\n",
    "    \\text{Octave: }\\texttt{pinv(X'*X)*X'*y}\\\\\n",
    "    \\text{Matlab: }\\texttt{pinv(transpose(X)*X)*transpose(X)*y}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "#### Non-Invertibility\n",
    "\n",
    "In the normal equation, what if $X^TX$ is non-invertible? This might happen due to linearly dependant features (e.g. house size in $ft^2$ and $m^2$) - these are reduntant features and only one must be chosen. Alternatively, this could also occur due to having too many features (i.e. $m \\ll n$) - this can be solved by deleting some trivial features or using *regularization* (discussed later).\n",
    "\n",
    "It turns out that both the Octave and Matlab codes still work with the `pinv` (pseudo-inverse) function, but not with the regular `inv` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Octave/Matlab Tutorial\n",
    "\n",
    "This section deals mainly with Octave - a lot of the commands/functions are the exact same in Matlab (refer to Matlab docs if you get stuck).\n",
    "\n",
    "#### Moving Data Around\n",
    "\n",
    "| Command | Function |\n",
    "| ------- | -------- |\n",
    "| `pwd` | print the current working directory |\n",
    "| `cd 'enter_path_here'` | change working directory to specified path (folders are separated by `:\\`) |\n",
    "| `ls` | list all folders/files in the working directory |\n",
    "| `load fileName.extension` | load all variables from specified file |\n",
    "| `who`/`whos` | print all variable names in the current workspace |\n",
    "| `clear variableName` | clears the speciefied variable from the workspace |\n",
    "| `save fileName.extension variableName` | saves the specified variable with the specified file name in the workng directory |\n",
    "| `matrix(:,column)`/`matrix(row,:)` | returns all elements from the specified row/column of a matrix |\n",
    "| `matrix = [matrix, column_vector]` | appends a column vector to the right of a matrix |\n",
    "| `matrix(:)` | combine all elements of a matrix into a single column vector |\n",
    "\n",
    "#### Computing on Data\n",
    "\n",
    "| Command | Function |\n",
    "| ------- | -------- |\n",
    "| `matrix1 .* matrix2` | returns an element-wise multiplication of two matrices |\n",
    "| `matrix'` | returns the transpose of a matrix |\n",
    "| `inv(matrix)`/`pinv(matrix)` | returns the inverse/pseudo-inverse of a matrix |\n",
    "\n",
    "#### Plotting Data\n",
    "\n",
    "| Command | Function |\n",
    "| ------- | -------- |\n",
    "| `plot(x, y)` | generates a plot of `y` against `x` |\n",
    "| `xlabel('string')`/`ylabel('string')` | adds labels to the X/Y axes of the plot |\n",
    "| `legend('string1', 'string2', ...)` | adds a legend to the plot |\n",
    "| `title('string')` | adds a title to the current plot |\n",
    "| `hold on` | enables generating multiple plots within the same figure |\n",
    "| `axis([xmin xmax ymin ymax]) ` | changes the visible range of the X and Y axes |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
