{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Week 3 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Representation\n",
    "\n",
    "Classification refers to any problem with discrete outputs - e.g. weather prediction (rainy? sunny? cloudy?), exam grades (A+, A, B, C, ...), email (spam? not spam?). One could implement linear regression on such a dataset and use a threshold classifier (i.e. if $h_{\\theta}$ is below a certain value, predict 0 and if above a certain value, predict 1). This is not a particularly good idea, since outliers in the set might make significant contributions to the threshold.\n",
    "\n",
    "For now, we focus on only the binary classification problem (i.e. $y$ can only take values 0 and 1).\n",
    "\n",
    "#### Hypothesis for Logistic Regression\n",
    "\n",
    "For linear regression, we used the hypothesis $h_{\\theta}(x) = \\theta^Tx$. For logistic regression, we make a slight change to this hypothesis by setting $h_{\\theta}(x) = g(\\theta^Tx)$ where $g$ is the sigmoid function.\n",
    "\n",
    "$$h_{\\theta}(x)=g(\\theta^Tx) = \\dfrac{1}{1 + e^{-\\theta^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
